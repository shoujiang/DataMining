{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bbc/business/005.txt\",\"r\") as f:\n",
    "    textString=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本规范化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pernod',\n",
       " 'takeover',\n",
       " 'talk',\n",
       " 'lifts',\n",
       " 'Domecq',\n",
       " 'Shares',\n",
       " 'UK',\n",
       " 'drinks',\n",
       " 'food',\n",
       " 'firm',\n",
       " 'Allied',\n",
       " 'Domecq',\n",
       " 'risen',\n",
       " 'speculation',\n",
       " 'could',\n",
       " 'target',\n",
       " 'takeover',\n",
       " 'France',\n",
       " 'Pernod',\n",
       " 'Ricard',\n",
       " 'Reports',\n",
       " 'Wall',\n",
       " 'Street',\n",
       " 'Journal',\n",
       " 'Financial',\n",
       " 'Times',\n",
       " 'suggested',\n",
       " 'French',\n",
       " 'spirits',\n",
       " 'firm',\n",
       " 'considering',\n",
       " 'bid',\n",
       " 'yet',\n",
       " 'contact',\n",
       " 'target',\n",
       " 'Allied',\n",
       " 'Domecq',\n",
       " 'shares',\n",
       " 'London',\n",
       " 'rose',\n",
       " 'GMT',\n",
       " 'Pernod',\n",
       " 'shares',\n",
       " 'Paris',\n",
       " 'slipped',\n",
       " 'Pernod',\n",
       " 'said',\n",
       " 'seeking',\n",
       " 'acquisitions',\n",
       " 'refused',\n",
       " 'comment',\n",
       " 'specifics',\n",
       " 'Pernod',\n",
       " 'last',\n",
       " 'major',\n",
       " 'purchase',\n",
       " 'third',\n",
       " 'US',\n",
       " 'giant',\n",
       " 'Seagram',\n",
       " 'move',\n",
       " 'propelled',\n",
       " 'global',\n",
       " 'top',\n",
       " 'drinks',\n",
       " 'firms',\n",
       " 'The',\n",
       " 'thirds',\n",
       " 'Seagram',\n",
       " 'bought',\n",
       " 'market',\n",
       " 'leader',\n",
       " 'Diageo',\n",
       " 'In',\n",
       " 'terms',\n",
       " 'market',\n",
       " 'value',\n",
       " 'Pernod',\n",
       " '5bn',\n",
       " 'euros',\n",
       " '7bn',\n",
       " 'smaller',\n",
       " 'Allied',\n",
       " 'Domecq',\n",
       " 'capitalisation',\n",
       " '7bn',\n",
       " '7bn',\n",
       " '2bn',\n",
       " 'euros',\n",
       " 'Last',\n",
       " 'year',\n",
       " 'Pernod',\n",
       " 'tried',\n",
       " 'buy',\n",
       " 'Glenmorangie',\n",
       " 'Scotland',\n",
       " 'premier',\n",
       " 'whisky',\n",
       " 'firms',\n",
       " 'lost',\n",
       " 'luxury',\n",
       " 'goods',\n",
       " 'firm',\n",
       " 'LVMH',\n",
       " 'Pernod',\n",
       " 'home',\n",
       " 'brands',\n",
       " 'including',\n",
       " 'Chivas',\n",
       " 'Regal',\n",
       " 'Scotch',\n",
       " 'whisky',\n",
       " 'Havana',\n",
       " 'Club',\n",
       " 'rum',\n",
       " 'Jacob',\n",
       " 'Creek',\n",
       " 'wine',\n",
       " 'Allied',\n",
       " 'Domecq',\n",
       " 'big',\n",
       " 'names',\n",
       " 'include',\n",
       " 'Malibu',\n",
       " 'rum',\n",
       " 'Courvoisier',\n",
       " 'brandy',\n",
       " 'Stolichnaya',\n",
       " 'vodka',\n",
       " 'Ballantine',\n",
       " 'whisky',\n",
       " 'well',\n",
       " 'snack',\n",
       " 'food',\n",
       " 'chains',\n",
       " 'Dunkin',\n",
       " 'Donuts',\n",
       " 'Baskin',\n",
       " 'Robbins',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'The',\n",
       " 'WSJ',\n",
       " 'said',\n",
       " 'ripe',\n",
       " 'consolidation',\n",
       " 'dealt',\n",
       " 'problematic',\n",
       " 'parts',\n",
       " 'portfolio',\n",
       " 'Pernod',\n",
       " 'reduced',\n",
       " 'debt',\n",
       " 'took',\n",
       " 'fund',\n",
       " 'Seagram',\n",
       " 'purchase',\n",
       " '8bn',\n",
       " 'euros',\n",
       " 'Allied',\n",
       " 'improved',\n",
       " 'performance',\n",
       " 'fast',\n",
       " 'food',\n",
       " 'chains']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also']\n",
    "# 分词\n",
    "def tokenize_text(text):\n",
    "    tokens=nltk.word_tokenize(text)#分词\n",
    "    tokens=[token.strip() for token in tokens]#去除单词前后的空格或换行字符\n",
    "    return tokens\n",
    "# 移除特殊字符\n",
    "def remove_special_characters(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    pattern=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    #re.compile函数根据包含的正则表达式的字符串创建模式对象,可以实现更有效率的匹配.\n",
    "    #re.escape(pattern) 可以对字符串中所有可能被解释为正则运算符的字符进行转义的应用函数\n",
    "    #string.punctuation找出字符串中的所有的标点\n",
    "    filtered_tokens=filter(None,[pattern.sub(' ',token) for token in tokens])\n",
    "    # filter过滤掉不符合条件的元素.pattern.sub实现比普通字符串replace 更加强大的替换功能\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "# 移除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    filtered_tokens=[token for token in tokens\n",
    "                    if token not in stopword_list]\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "# keep text characters\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens=[]\n",
    "    tokens=tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "text = textString\n",
    "text=remove_special_characters(text)\n",
    "text=remove_stopwords(text)\n",
    "text=keep_text_characters(text)\n",
    "text=tokenize_text(text)\n",
    "# normalized_corpus=[]\n",
    "# normalized_corpus.append(text)\n",
    "# normalized_corpus\n",
    "# text=' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "shingling_2=list(ngrams(text, 20))\n",
    "shingling_3=list(ngrams(text, 30))\n",
    "shingling_4=list(ngrams(text, 40))\n",
    "# shingling_5=list(ngrams(text, 11))\n",
    "data=shingling_2+shingling_3+shingling_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用mlxtend包进行挖掘频繁项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727941</td>\n",
       "      <td>(Allied)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691176</td>\n",
       "      <td>(Domecq)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>(Pernod)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669118</td>\n",
       "      <td>(Domecq, Allied)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637255</td>\n",
       "      <td>(Pernod, Allied)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.595588</td>\n",
       "      <td>(Domecq, Pernod)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>(Domecq, Pernod, Allied)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support                  itemsets\n",
       "0  0.727941                  (Allied)\n",
       "1  0.691176                  (Domecq)\n",
       "2  0.862745                  (Pernod)\n",
       "3  0.669118          (Domecq, Allied)\n",
       "4  0.637255          (Pernod, Allied)\n",
       "5  0.595588          (Domecq, Pernod)\n",
       "6  0.578431  (Domecq, Pernod, Allied)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from apyori import apriori\n",
    "# result= list(apriori(transactions=data,min_support=0.4))\n",
    "# print(result)\n",
    "# print(data)\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "# 测试数据\n",
    "# data = [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n",
    "# data = [('豆奶','莴苣'),\n",
    "#         ('莴苣','尿布','葡萄酒','甜菜'),\n",
    "#         ('豆奶','尿布','葡萄酒','橙汁'),\n",
    "#         ('莴苣','豆奶','尿布','葡萄酒'),\n",
    "#         ('莴苣','豆奶','尿布','橙汁')]\n",
    "# def encode_units(x):\n",
    "#     if x <= 0:\n",
    "#         return 0\n",
    "#     if x >= 1:\n",
    "#         return 1\n",
    "# data = data.applymap(encode_units)\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "a_rules = association_rules(frequent_itemsets, min_threshold=0.1)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>(Allied)</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>1.329895</td>\n",
       "      <td>0.165982</td>\n",
       "      <td>8.524510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Allied)</td>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>1.329895</td>\n",
       "      <td>0.165982</td>\n",
       "      <td>3.821691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>(Allied)</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>1.014692</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>1.040921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Allied)</td>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>1.014692</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>1.101749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.992459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.690341</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.997302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Domecq, Pernod)</td>\n",
       "      <td>(Allied)</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>1.334165</td>\n",
       "      <td>0.144878</td>\n",
       "      <td>9.444328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Domecq, Allied)</td>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.864469</td>\n",
       "      <td>1.001998</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>1.012719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Pernod, Allied)</td>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>1.313257</td>\n",
       "      <td>0.137976</td>\n",
       "      <td>3.345588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Domecq)</td>\n",
       "      <td>(Pernod, Allied)</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>1.313257</td>\n",
       "      <td>0.137976</td>\n",
       "      <td>2.223785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(Pernod)</td>\n",
       "      <td>(Domecq, Allied)</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>1.001998</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>1.004057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(Allied)</td>\n",
       "      <td>(Domecq, Pernod)</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>1.334165</td>\n",
       "      <td>0.144878</td>\n",
       "      <td>1.969021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         antecedents       consequents  antecedent support  \\\n",
       "0           (Domecq)          (Allied)            0.691176   \n",
       "1           (Allied)          (Domecq)            0.727941   \n",
       "2           (Pernod)          (Allied)            0.862745   \n",
       "3           (Allied)          (Pernod)            0.727941   \n",
       "4           (Domecq)          (Pernod)            0.691176   \n",
       "5           (Pernod)          (Domecq)            0.862745   \n",
       "6   (Domecq, Pernod)          (Allied)            0.595588   \n",
       "7   (Domecq, Allied)          (Pernod)            0.669118   \n",
       "8   (Pernod, Allied)          (Domecq)            0.637255   \n",
       "9           (Domecq)  (Pernod, Allied)            0.691176   \n",
       "10          (Pernod)  (Domecq, Allied)            0.862745   \n",
       "11          (Allied)  (Domecq, Pernod)            0.727941   \n",
       "\n",
       "    consequent support   support  confidence      lift  leverage  conviction  \n",
       "0             0.727941  0.669118    0.968085  1.329895  0.165982    8.524510  \n",
       "1             0.691176  0.669118    0.919192  1.329895  0.165982    3.821691  \n",
       "2             0.727941  0.637255    0.738636  1.014692  0.009227    1.040921  \n",
       "3             0.862745  0.637255    0.875421  1.014692  0.009227    1.101749  \n",
       "4             0.862745  0.595588    0.861702  0.998791 -0.000721    0.992459  \n",
       "5             0.691176  0.595588    0.690341  0.998791 -0.000721    0.997302  \n",
       "6             0.727941  0.578431    0.971193  1.334165  0.144878    9.444328  \n",
       "7             0.862745  0.578431    0.864469  1.001998  0.001153    1.012719  \n",
       "8             0.691176  0.578431    0.907692  1.313257  0.137976    3.345588  \n",
       "9             0.637255  0.578431    0.836879  1.313257  0.137976    2.223785  \n",
       "10            0.669118  0.578431    0.670455  1.001998  0.001153    1.004057  \n",
       "11            0.595588  0.578431    0.794613  1.334165  0.144878    1.969021  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ariori实现频繁项挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'Pernod'}): 0.8627450980392157,\n",
       " frozenset({'Domecq'}): 0.6911764705882353,\n",
       " frozenset({'Allied'}): 0.7279411764705882,\n",
       " frozenset({'Allied', 'Domecq'}): 0.6691176470588235,\n",
       " frozenset({'Domecq', 'Pernod'}): 0.5955882352941176,\n",
       " frozenset({'Allied', 'Pernod'}): 0.6372549019607843,\n",
       " frozenset({'Allied', 'Domecq', 'Pernod'}): 0.5784313725490197}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_freq_supports(data_set,item_set,min_support):\n",
    "    freq_set = set() # 保存频繁项集元素\n",
    "    item_count = {} # 保存元素频次，用来计算支持度\n",
    "    supports = {} # 保存支持度\n",
    "    # 如果项集中的元素再数据集中则计数\n",
    "    for record in data_set:\n",
    "        for item in item_set:\n",
    "            if item.issubset(record):\n",
    "                if item not in item_count:\n",
    "                    item_count[item] = 1\n",
    "                else:\n",
    "                    item_count[item] += 1\n",
    "    data_len = float(len(data_set))\n",
    "    # 计算项集的支持度\n",
    "    for item in item_count:\n",
    "        if (item_count[item]/data_len) >= min_support:\n",
    "            freq_set.add(item)\n",
    "            supports[item] = item_count[item]/data_len\n",
    "            \n",
    "    return freq_set,supports\n",
    "\n",
    "# 生成新组合，由初始候选集生成\n",
    "def generate_new_combinations(freq_set,k):\n",
    "    new_combinations = set() # 保存新组合\n",
    "    sets_len = len(freq_set) # 集合含有元素的个数，用于遍历求得组合\n",
    "    freq_set_list = list(freq_set) #集合转为列表用于索引\n",
    "    \n",
    "    for i in range(sets_len):\n",
    "        for j in range(i+1,sets_len):\n",
    "            l1 = list(freq_set_list[i])\n",
    "            l2 = list(freq_set_list[j])\n",
    "            l1.sort()\n",
    "            l2.sort()\n",
    "            # 项集若有相同的父集则合并项集\n",
    "            if l1[0:k-2] == l2[0:k-2]:\n",
    "                freq_item = freq_set_list[i]|freq_set_list[j]\n",
    "                new_combinations.add(freq_item)\n",
    "    return new_combinations\n",
    "def apriori(data_set,min_support,max_len=None):\n",
    "    max_items = 2 # 初始项集元素的个数\n",
    "    freq_sets = [] # 保存所有的频繁项集\n",
    "    supports = {} # 保存所有的支持度\n",
    "    # 候选项1项集\n",
    "    c1 = set()\n",
    "    for items in data_set:\n",
    "        for item in items:\n",
    "            item_set = frozenset([item])#frozenset() 冻结后集合不能再添加或删除任何元素。为了后续计算支持度字典是将集合作为键\n",
    "            c1.add(item_set) \n",
    "#     print('c1:',c1)\n",
    "#     print('\\n')\n",
    "    # 频繁项1项集及其支持度\n",
    "    l1,support1 = generate_freq_supports(data_set,c1,min_support)\n",
    "    freq_sets.append(l1)\n",
    "    supports.update(support1)\n",
    "#     print('频繁项1项集:',freq_sets[-1])\n",
    "#     print('频繁项1项集:',freq_sets)\n",
    "#     print('支持度:',supports)\n",
    "#     print('\\n')\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = float('inf')\n",
    "        \n",
    "    while max_items and max_items <= max_len:\n",
    "        ci = generate_new_combinations(freq_sets[-1],max_items) #生成候选集\n",
    "        li,support = generate_freq_supports(data_set,ci,min_support) # 生成频繁项集和支持度\n",
    "#         print('生成候选集 ci',ci)\n",
    "#         print('\\n')\n",
    "        # 如果由频繁项及则进入下个循环\n",
    "        if li:\n",
    "            freq_sets.append(li)\n",
    "            supports.update(support)\n",
    "            max_items += 1\n",
    "        else:\n",
    "            max_items = 0\n",
    "    return freq_sets,supports    \n",
    "def association_reles(freq_sets,supports,min_conf):\n",
    "    rules = []\n",
    "    max_len = len(freq_sets)\n",
    "    \n",
    "    # 生成关联规则，筛选符合规则的频繁项集计算置信度，满足最小置信度的关联规则添加到列表\n",
    "    for k in range(max_len - 1):\n",
    "        for freq_set in freq_sets[k]:\n",
    "            for sub_set in freq_sets[k + 1]:\n",
    "                if freq_set.issubset(sub_set):\n",
    "                    conf = supports[sub_set] / supports[freq_set]\n",
    "                    rule = (freq_set, sub_set - freq_set, conf)\n",
    "                    if conf >= min_conf:\n",
    "                        rules.append(rule)\n",
    "    return rules\n",
    "L,support_data = apriori(data, min_support=0.5)\n",
    "rules = association_reles(L,support_data,0.1)\n",
    "# print(L)\n",
    "support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'Domecq'}), frozenset({'Allied'}), 0.9680851063829786),\n",
       " (frozenset({'Domecq'}), frozenset({'Pernod'}), 0.8617021276595744),\n",
       " (frozenset({'Pernod'}), frozenset({'Domecq'}), 0.6903409090909091),\n",
       " (frozenset({'Pernod'}), frozenset({'Allied'}), 0.7386363636363635),\n",
       " (frozenset({'Allied'}), frozenset({'Domecq'}), 0.9191919191919191),\n",
       " (frozenset({'Allied'}), frozenset({'Pernod'}), 0.8754208754208754),\n",
       " (frozenset({'Allied', 'Domecq'}), frozenset({'Pernod'}), 0.8644688644688646),\n",
       " (frozenset({'Domecq', 'Pernod'}), frozenset({'Allied'}), 0.9711934156378602),\n",
       " (frozenset({'Allied', 'Pernod'}), frozenset({'Domecq'}), 0.9076923076923078)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
