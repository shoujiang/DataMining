{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also']\n",
    "# 分词\n",
    "def tokenize_text(text):\n",
    "    tokens=nltk.word_tokenize(text)#分词\n",
    "    tokens=[token.strip() for token in tokens]#去除单词前后的空格或换行字符\n",
    "    return tokens\n",
    "# 移除特殊字符\n",
    "def remove_special_characters(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    pattern=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    #re.compile函数根据包含的正则表达式的字符串创建模式对象,可以实现更有效率的匹配.\n",
    "    #re.escape(pattern) 可以对字符串中所有可能被解释为正则运算符的字符进行转义的应用函数\n",
    "    #string.punctuation找出字符串中的所有的标点\n",
    "    filtered_tokens=filter(None,[pattern.sub(' ',token) for token in tokens])\n",
    "    # filter过滤掉不符合条件的元素.pattern.sub实现比普通字符串replace 更加强大的替换功能\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "# 移除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    filtered_tokens=[token for token in tokens\n",
    "                    if token not in stopword_list]\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "# keep text characters\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens=[]\n",
    "    tokens=tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    " \n",
    "def to_unicode_or_bust(obj,encoding='utf-8'):\n",
    "    if isinstance(obj, basestring):\n",
    "        if not isinstance(obj, unicode):\n",
    "            obj = unicode(obj, encoding)\n",
    "    return obj\n",
    " \n",
    "\"\"\"\n",
    "此函数用于获取dir文件夹中的文件的内容\n",
    "\"\"\"\n",
    "def getFilesName(dir):\n",
    "    fileList=[]\n",
    "    t=os.walk(dir)\n",
    "    file=dir+'\\\\'\n",
    "    for item in t:\n",
    "        for name in item[2]:\n",
    "            fileList.append(file+name)\n",
    "    return fileList\n",
    " \n",
    "\"\"\"\n",
    "此函数用于获得fileName文件中的内容，文件内容存放在字符串中返回\n",
    "\"\"\"\n",
    "def getFileContent(fileName):\n",
    "#     file=open(fileName,\"r\")\n",
    "#     fileContent=file.read()\n",
    "#     fileContent=fileContent.replace(\"\\t\",\" \")\n",
    "#     fileContent=fileContent.replace(\"\\n\",\" \")\n",
    "#     fileContent=fileContent.replace(\"\\r\",\" \")\n",
    "#     file.close()\n",
    "#     print(fileContent)\n",
    "#     print('*'*50)\n",
    "    with open(fileName,\"r\") as f:\n",
    "        fileContent=f.read()\n",
    "    return fileContent\n",
    "#return fileConten.decode(\"gb2312\").encode(\"utf-8\")用于显示\n",
    " \n",
    "\"\"\"\n",
    "此函数用于对各个文件中的内容进行k-shingle，然后对词条进行哈希（此处就用字典存储了）\n",
    "其中dir是文件夹的名称字符串类型，k是int型\n",
    "\"\"\"\n",
    "from nltk.util import ngrams\n",
    "def getShingleList(dir,k):\n",
    "    fileList=getFilesName(dir)\n",
    "    shingleList=list()\n",
    "    for fileName in fileList:\n",
    "        fileContent=getFileContent(fileName)\n",
    "        text=fileContent\n",
    "        text=remove_special_characters(text)\n",
    "        text=remove_stopwords(text)\n",
    "        text=keep_text_characters(text)\n",
    "        text=tokenize_text(text)\n",
    "        text=list(ngrams(text, k))\n",
    "        shingle = set()\n",
    "        for t in text:\n",
    "            shingle.add(' '.join(list(t)))       \n",
    "#         for index in range(0,len(fileContent)-k+1):\n",
    "#             shingle.add(fileContent[index:index+k])\n",
    "#         print(shingle)\n",
    "#         print('***'*50)\n",
    "        \n",
    "        \n",
    "#         print(shingle)\n",
    "#         print('#'*100)\n",
    "        shingleList.append(shingle)\n",
    "#     print(shingleList)\n",
    "#     print('='*100)\n",
    "    return shingleList\n",
    " \n",
    "\"\"\"\n",
    "此处是新版的函数，将哈希签名的矩阵换的行列换了一下，便于接下来使用\n",
    "\"\"\"\n",
    "def getMinHashSignature(shingleList,signatureNum):\n",
    "    #tatalSet用于存放所有集合的并集\n",
    "    totalSet=shingleList[0]\n",
    "    for i in range(1,len(shingleList)):\n",
    "        totalSet=totalSet|shingleList[i]\n",
    " \n",
    "    temp=int(math.sqrt(signatureNum))\n",
    "    #randomArray用于模拟随机哈希函数\n",
    "    randomArray=[]\n",
    "    #signatureList用于存放总的哈希签名\n",
    "    signatureList=[]\n",
    "    maxNum=sys.maxsize\n",
    "    for i in range(signatureNum):\n",
    "        randomArray.append(random.randint(1,temp*2))\n",
    "        randomArray.append(random.randint(1,temp*2))\n",
    "    #buketNum用于记录所有元素的个数，作为随机哈希函数的桶号\n",
    "    buketNum=len(totalSet)\n",
    "    \"\"\"\n",
    "    此处将不同文档的自己的哈希签名存成一个list，然后再进行汇总到一个总的list\n",
    "    \"\"\"\n",
    "    for shingleSet in shingleList:\n",
    "        \"\"\"\n",
    "        signature用于存放哈希函数产生的签名\n",
    "        \"\"\"\n",
    "        signature=[]\n",
    "        for i in range(signatureNum):\n",
    "            minHash=maxNum\n",
    "            for index,item in enumerate(totalSet):\n",
    "                if item in shingleSet:\n",
    "                    num=(randomArray[i*2]*index+randomArray[i*2+1])%buketNum\n",
    "                    minHash=min(minHash,num)\n",
    "            signature.append(minHash)\n",
    "        signatureList.append(signature)\n",
    "    return signatureList\n",
    " \n",
    "\"\"\"\n",
    "在使用局部敏感哈希中，我们假设行条(也叫分组)和每个行条中行的个数的乘积刚好等于总的签名数，这样可以减少不必要的繁琐，\n",
    "这就要求我们在选取相关参数时要注意\n",
    "\"\"\"\n",
    "def localSensitiveHash(signatureList,filesName,signatureNum,bands):\n",
    "    lshResult=[]\n",
    "    \"\"\"\n",
    "    此循环用于初始化这个list，构造一个下三角的模拟数组，用于存放两个文件之间行的相似个数\n",
    "    \"\"\"\n",
    "    for i in range(len(signatureList)):\n",
    "        temp=[]\n",
    "        for j in range(i):\n",
    "            temp.append(0)\n",
    "        lshResult.append(temp)\n",
    " \n",
    "    row=int(signatureNum/bands)\n",
    "    #此循环是对签名的行条进行处理\n",
    "    for i in range(0,bands):\n",
    "        dicResult={}\n",
    "        index=i*row\n",
    "        #此循环是对行条中的每一列进行计算，num用于记录签名的下标\n",
    "        for num,signature in enumerate(signatureList):\n",
    "            #hashCode用于记录每个行条的hash桶号\n",
    "            hashCode = 0\n",
    "            #遍历行条中的列，此处的hash函数就暂时用累加，稍后修改\n",
    "            for j in range(index,index+row):\n",
    "                hashCode+=signature[j]\n",
    "            if hashCode in dicResult:\n",
    "                dicResult[hashCode].append(num)\n",
    "            else:\n",
    "                dicResult[hashCode]=[num]\n",
    "        for valueList in dicResult.values():\n",
    "            #print valueList\n",
    "            if(len(valueList)>=2):\n",
    "                for index1 in range(len(valueList)):\n",
    "                    for index2 in range(index1+1,len(valueList)):\n",
    "                        lshResult[valueList[index2]][valueList[index1]]+=1\n",
    " \n",
    "    similarityResult=[]\n",
    "    for i in range(1,len(lshResult)):\n",
    "        for j in range(len(lshResult[i])):\n",
    "            similarity=lshResult[i][j]/(bands*1.0)\n",
    "            similarityResult.append((similarity,filesName[i],filesName[j]))\n",
    "    return similarityResult\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, './bbc/test\\\\011.txt', './bbc/test\\\\001.txt')\n",
      "(0.025, './bbc/test\\\\011.txt', './bbc/test\\\\010.txt')\n",
      "(0.025, './bbc/test\\\\011.txt', './bbc/test\\\\007.txt')\n",
      "(0.025, './bbc/test\\\\011.txt', './bbc/test\\\\004.txt')\n",
      "(0.025, './bbc/test\\\\011.txt', './bbc/test\\\\002.txt')\n",
      "(0.025, './bbc/test\\\\010.txt', './bbc/test\\\\001.txt')\n",
      "(0.025, './bbc/test\\\\009.txt', './bbc/test\\\\007.txt')\n",
      "(0.025, './bbc/test\\\\007.txt', './bbc/test\\\\005.txt')\n",
      "(0.025, './bbc/test\\\\007.txt', './bbc/test\\\\001.txt')\n",
      "(0.025, './bbc/test\\\\005.txt', './bbc/test\\\\004.txt')\n",
      "(0.025, './bbc/test\\\\004.txt', './bbc/test\\\\002.txt')\n",
      "(0.025, './bbc/test\\\\004.txt', './bbc/test\\\\001.txt')\n",
      "(0.025, './bbc/test\\\\002.txt', './bbc/test\\\\001.txt')\n",
      "(0.0, './bbc/test\\\\011.txt', './bbc/test\\\\009.txt')\n",
      "(0.0, './bbc/test\\\\011.txt', './bbc/test\\\\008.txt')\n",
      "(0.0, './bbc/test\\\\011.txt', './bbc/test\\\\006.txt')\n",
      "(0.0, './bbc/test\\\\011.txt', './bbc/test\\\\005.txt')\n",
      "(0.0, './bbc/test\\\\011.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\009.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\008.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\007.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\006.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\005.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\004.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\010.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\008.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\006.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\005.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\004.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\009.txt', './bbc/test\\\\001.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\007.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\006.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\005.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\004.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\008.txt', './bbc/test\\\\001.txt')\n",
      "(0.0, './bbc/test\\\\007.txt', './bbc/test\\\\006.txt')\n",
      "(0.0, './bbc/test\\\\007.txt', './bbc/test\\\\004.txt')\n",
      "(0.0, './bbc/test\\\\007.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\007.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\006.txt', './bbc/test\\\\005.txt')\n",
      "(0.0, './bbc/test\\\\006.txt', './bbc/test\\\\004.txt')\n",
      "(0.0, './bbc/test\\\\006.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\006.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\006.txt', './bbc/test\\\\001.txt')\n",
      "(0.0, './bbc/test\\\\005.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\005.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\005.txt', './bbc/test\\\\001.txt')\n",
      "(0.0, './bbc/test\\\\004.txt', './bbc/test\\\\003.txt')\n",
      "(0.0, './bbc/test\\\\003.txt', './bbc/test\\\\002.txt')\n",
      "(0.0, './bbc/test\\\\003.txt', './bbc/test\\\\001.txt')\n"
     ]
    }
   ],
   "source": [
    "dir=\"./bbc/test\"            # 存放文档的目录路径\n",
    "filesName=getFilesName(dir)\n",
    "shingleList=getShingleList(dir,2)\n",
    "signatureList=getMinHashSignature(shingleList,200)\n",
    "result=localSensitiveHash(signatureList,filesName,200,40)\n",
    "result.sort(reverse=True)\n",
    "for each in result:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
